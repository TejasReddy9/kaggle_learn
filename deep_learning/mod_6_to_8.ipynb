{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "dfdf80a5-bb2a-41c8-bb66-1e432e852b67",
        "_uuid": "6cc8f4d4b8d92d1970c259922d95cf17f80a7487"
      },
      "cell_type": "markdown",
      "source": "# Introduction\n\nYou've seen how to build a model from scratch to identify handwritten digits.  You'll now build a model to identify different types of clothing.  To make models that train quickly, we'll work with very small (low-resolution) images. \n\nAs an example, your model will take an images like this and identify it as a shoe:\n![Imgur](https://i.imgur.com/GyXOnSB.png)"
    },
    {
      "metadata": {
        "_uuid": "85022d62f8b2f581b2a067c8289fd67bcf517ccd"
      },
      "cell_type": "markdown",
      "source": "# Data Preparation\nThis code is supplied, and you don't need to change it. Just run the cell below."
    },
    {
      "metadata": {
        "_cell_guid": "d9aa6241-74fc-4c05-bad5-6281d3f45966",
        "_uuid": "2da7ce8c4a1de3b53665c145a91d4bc7e168ddd2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python import keras\n\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\ndef prep_data(raw, train_size, val_size):\n    y = raw[:, 0]\n    out_y = keras.utils.to_categorical(y, num_classes)\n    \n    x = raw[:,1:]\n    num_images = raw.shape[0]\n    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n    out_x = out_x / 255\n    return out_x, out_y\n\nfashion_file = \"../input/fashionmnist/fashion-mnist_train.csv\"\nfashion_data = np.loadtxt(fashion_file, skiprows=1, delimiter=',')\nx, y = prep_data(fashion_data, train_size=50000, val_size=5000)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0c6bbeef0057720505454507ae098d96b160b4df"
      },
      "cell_type": "markdown",
      "source": "# Specify Model\n**STEPS:**\n1. Create a `Sequential` model. Call it `fashion_model`.\n2. Add 3 `Conv2D` layers to `fashion_model`.  Make each layer have 12 filters, a kernel_size of 3 and a **relu** activation.  You will need to specify the `input_shape` for the first `Conv2D` layer.  The input shape in this case is `(img_rows, img_cols, 1)`.\n3. Add a `Flatten` layer to `fashion_model` after the last `Conv2D` layer.\n4. Add a `Dense` layer with 100 neurons to `fashion_model` after the `Flatten` layer.  \n5. Add your prediction layer to `fashion_model`.  This is a `Dense` layer.  We alrady have a variable called `num_classes`.  Use this variable when specifying the number of nodes in this layer. The activation should be `softmax` (or you will have problems later)."
    },
    {
      "metadata": {
        "_cell_guid": "d2e326a6-ac8a-422b-8cff-6c1d7ccd7ced",
        "_uuid": "4d2c616eb22814d5285ba267bc839a43b3fea6b6",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n\nfashion_model = Sequential()\nfashion_model.add(Conv2D(12, kernel_size=(3,3), strides=2,\n                         activation=\"relu\", \n                         input_shape=(img_rows, img_cols, 1), \n                         name=\"conv1\"))\nfashion_model.add(Conv2D(12, kernel_size=(3,3), strides=2,\n                         activation=\"relu\", \n                         name=\"conv2\"))\nfashion_model.add(Conv2D(12, kernel_size=(3,3), strides=2,\n                         activation=\"relu\", \n                         name=\"conv3\"))\nfashion_model.add(Flatten())\nfashion_model.add(Dense(100, activation=\"relu\", name=\"dense1\"))\nfashion_model.add(Dense(num_classes, activation=\"softmax\", name=\"dense2\"))",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6caaba14327fd7180378f3bb56a2661e09dcdd35"
      },
      "cell_type": "markdown",
      "source": "Let's see without strides."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "34bd41b072a23c2cec6464017063e81bddc4942d"
      },
      "cell_type": "code",
      "source": "fashion_model_without_strides = Sequential()\nfashion_model_without_strides.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         input_shape=(img_rows, img_cols, 1), \n                         name=\"conv1\"))\nfashion_model_without_strides.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         name=\"conv2\"))\nfashion_model_without_strides.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         name=\"conv3\"))\nfashion_model_without_strides.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         name=\"conv4\"))\nfashion_model_without_strides.add(Flatten())\nfashion_model_without_strides.add(Dense(100, activation=\"relu\", name=\"dense1\"))\nfashion_model_without_strides.add(Dense(num_classes, activation=\"softmax\", name=\"dense2\"))",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0caf87c4cfb1c5a21f87b9d12107de53e5ebb3c6"
      },
      "cell_type": "markdown",
      "source": "Dropout layers when added."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51d49b609ec6cdd31a32d25ae4904929b894755a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fashion_model_with_dropout = Sequential()\nfashion_model_with_dropout.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         input_shape=(img_rows, img_cols, 1), \n                         name=\"conv1\"))\nfashion_model_with_dropout.add(Dropout(0.5))\nfashion_model_with_dropout.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         name=\"conv2\"))\nfashion_model_with_dropout.add(Dropout(0.5))\nfashion_model_with_dropout.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         name=\"conv3\"))\nfashion_model_with_dropout.add(Dropout(0.5))\nfashion_model_with_dropout.add(Conv2D(24, kernel_size=(3,3),\n                         activation=\"relu\", \n                         name=\"conv4\"))\nfashion_model_with_dropout.add(Dropout(0.5))\nfashion_model_with_dropout.add(Flatten())\nfashion_model_with_dropout.add(Dense(100, activation=\"relu\", name=\"dense1\"))\nfashion_model_with_dropout.add(Dense(num_classes, activation=\"softmax\", name=\"dense2\"))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5e660e46f096b8adfeeb5660d179e81a40338e40"
      },
      "cell_type": "markdown",
      "source": "# Compile Model\nRun the command `fashion_model.compile`.  Specify the following arguments:\n1. `loss = keras.losses.categorical_crossentropy`\n2. `optimizer = 'adam'`\n3. `metrics = ['accuracy']`"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "e74bb650acce4efaa5335f6e48803f246332c679"
      },
      "cell_type": "code",
      "source": "fashion_model.compile(optimizer = \"adam\", loss=keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e0ff1e5e713d5fc86b279b439e0449aae03b0eba"
      },
      "cell_type": "code",
      "source": "fashion_model_without_strides.compile(optimizer = \"adam\", loss=keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c08d306e4053884e4e934b56d2d61fd62309b4b0"
      },
      "cell_type": "code",
      "source": "fashion_model_with_dropout.compile(optimizer = \"adam\", loss=keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ab679642ebe0a4edc511b361e1d9e1a4ab268f4"
      },
      "cell_type": "markdown",
      "source": "# Fit Model\nRun the command `fashion_model.fit`. The arguments you will use are\n1. The first two are arguments are the data used to fit the model, which are `x` and `y` respectively.\n2. `batch_size = 100`\n3. `epochs = 4`\n4. `validation_split = 0.2`\n\nWhen you run this command, you can watch your model start improving.  You will see validation accuracies after each epoch."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a001324c9bc11570a15eb201070d65cd8d0e81da"
      },
      "cell_type": "code",
      "source": "fashion_model.summary()\nfashion_model.fit(x, y, \n                  batch_size=100, \n                  epochs=4, \n                  validation_split=0.2)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1 (Conv2D)               (None, 13, 13, 12)        120       \n_________________________________________________________________\nconv2 (Conv2D)               (None, 6, 6, 12)          1308      \n_________________________________________________________________\nconv3 (Conv2D)               (None, 2, 2, 12)          1308      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 48)                0         \n_________________________________________________________________\ndense1 (Dense)               (None, 100)               4900      \n_________________________________________________________________\ndense2 (Dense)               (None, 10)                1010      \n=================================================================\nTotal params: 8,646\nTrainable params: 8,646\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/4\n48000/48000 [==============================] - 4s 74us/step - loss: 0.4604 - acc: 0.8291 - val_loss: 0.4745 - val_acc: 0.8270\nEpoch 2/4\n48000/48000 [==============================] - 4s 75us/step - loss: 0.4437 - acc: 0.8322 - val_loss: 0.4508 - val_acc: 0.8372\nEpoch 3/4\n48000/48000 [==============================] - 4s 76us/step - loss: 0.4289 - acc: 0.8396 - val_loss: 0.4407 - val_acc: 0.8384\nEpoch 4/4\n48000/48000 [==============================] - 4s 74us/step - loss: 0.4182 - acc: 0.8430 - val_loss: 0.4332 - val_acc: 0.8387\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f78dcd99908>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9ed4e1bc78b646701f0fab2cc526dfc09e5e622"
      },
      "cell_type": "code",
      "source": "fashion_model_without_strides.summary()\nfashion_model_without_strides.fit(x, y, \n                  batch_size=100, \n                  epochs=4, \n                  validation_split=0.2)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1 (Conv2D)               (None, 26, 26, 24)        240       \n_________________________________________________________________\nconv2 (Conv2D)               (None, 24, 24, 24)        5208      \n_________________________________________________________________\nconv3 (Conv2D)               (None, 22, 22, 24)        5208      \n_________________________________________________________________\nconv4 (Conv2D)               (None, 20, 20, 24)        5208      \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 9600)              0         \n_________________________________________________________________\ndense1 (Dense)               (None, 100)               960100    \n_________________________________________________________________\ndense2 (Dense)               (None, 10)                1010      \n=================================================================\nTotal params: 976,974\nTrainable params: 976,974\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/4\n48000/48000 [==============================] - 9s 180us/step - loss: 0.4843 - acc: 0.8256 - val_loss: 0.3484 - val_acc: 0.8747\nEpoch 2/4\n48000/48000 [==============================] - 8s 174us/step - loss: 0.3074 - acc: 0.8868 - val_loss: 0.2951 - val_acc: 0.8946\nEpoch 3/4\n48000/48000 [==============================] - 8s 174us/step - loss: 0.2473 - acc: 0.9089 - val_loss: 0.2563 - val_acc: 0.9085\nEpoch 4/4\n48000/48000 [==============================] - 8s 167us/step - loss: 0.2051 - acc: 0.9238 - val_loss: 0.2584 - val_acc: 0.9104\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f78dcd99e10>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48f6e18c72de8f248f1fce96b87a9544c0075447"
      },
      "cell_type": "code",
      "source": "fashion_model_with_dropout.summary()\nfashion_model_with_dropout.fit(x, y, \n                  batch_size=100, \n                  epochs=4, \n                  validation_split=0.2)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1 (Conv2D)               (None, 26, 26, 24)        240       \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 26, 26, 24)        0         \n_________________________________________________________________\nconv2 (Conv2D)               (None, 24, 24, 24)        5208      \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 24, 24, 24)        0         \n_________________________________________________________________\nconv3 (Conv2D)               (None, 22, 22, 24)        5208      \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 22, 22, 24)        0         \n_________________________________________________________________\nconv4 (Conv2D)               (None, 20, 20, 24)        5208      \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 20, 20, 24)        0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 9600)              0         \n_________________________________________________________________\ndense1 (Dense)               (None, 100)               960100    \n_________________________________________________________________\ndense2 (Dense)               (None, 10)                1010      \n=================================================================\nTotal params: 976,974\nTrainable params: 976,974\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/4\n48000/48000 [==============================] - 12s 240us/step - loss: 0.6549 - acc: 0.7552 - val_loss: 0.4442 - val_acc: 0.8405\nEpoch 2/4\n48000/48000 [==============================] - 11s 226us/step - loss: 0.4424 - acc: 0.8350 - val_loss: 0.3966 - val_acc: 0.8557\nEpoch 3/4\n48000/48000 [==============================] - 11s 228us/step - loss: 0.3841 - acc: 0.8567 - val_loss: 0.3338 - val_acc: 0.8779\nEpoch 4/4\n48000/48000 [==============================] - 11s 236us/step - loss: 0.3446 - acc: 0.8704 - val_loss: 0.3001 - val_acc: 0.8919\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f78d147f908>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7e9d2b005c4f0fb06be1a177f9fd47c1c4a6abd4"
      },
      "cell_type": "markdown",
      "source": "Observe the running times of these model, fork this notebook and see yourself."
    },
    {
      "metadata": {
        "_uuid": "7190e55038bb42c8c1818cc0d6e55e2f02a8092e"
      },
      "cell_type": "markdown",
      "source": "# Keep Going\n\n---\n**[Deep Learning Track Home Page](https://www.kaggle.com/learn/deep-learning)**\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}